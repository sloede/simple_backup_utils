#!/usr/bin/env python3

DEFAULT_CONFIG_DIR = '.sbu'
UPLOAD_PART_SIZE_MB = 8

import argparse
import binascii
import boto3
import configparser
import hashlib
import json
import os
import pprint
import sys
import time


def main():
    # Parse command line arguments and read configuration file
    args = parse_arguments()
    config = configparser.ConfigParser()
    config.read(os.path.join(args.config_dir, 'config'))

    # Read AWS credentials file
    aws_credentials = configparser.ConfigParser()
    aws_credentials.read(config['DEFAULT']['aws_credentials_file'])

    # Create boto3 client for AWS Glacier
    glacier = boto3.client(
            'glacier',
            aws_access_key_id=aws_credentials['default']['aws_access_key_id'],
            aws_secret_access_key=aws_credentials['default']['aws_secret_access_key'])

    # Upload all files found on command line
    for filename in args.filename:
        # Upload archive
        archive = upload_archive(filename, config['DEFAULT']['aws_vault_name'],
                glacier)

        # Print archive information
        print(json.dumps(archive, sort_keys=True))


def upload_archive(filename, vault, glacier_client,
        part_size=UPLOAD_PART_SIZE_MB*1024*1024):
    # Initiate multipart upload
    upload_info = glacier_client.initiate_multipart_upload(
            vaultName=vault,
            archiveDescription=os.path.basename(filename),
            partSize=str(part_size))

    # Upload file in parts
    file_size = 0
    hashes = []
    with open(filename, 'rb') as f:
        for part in iter(lambda: f.read(part_size), b''):
            # Get tree hash
            tree_hash = tree_hash_bytes(part)

            # Upload part
            part_info = glacier_client.upload_multipart_part(
                    vaultName=vault,
                    uploadId=upload_info['uploadId'],
                    checksum=bin_to_hex(tree_hash),
                    range="bytes {}-{}/*".format(file_size, file_size + len(part) - 1),
                    body=part)

            # Update file size and hash list
            file_size = file_size + len(part)
            hashes.append(tree_hash)

    # Finalize upload
    archive_info = glacier_client.complete_multipart_upload(
            vaultName=vault,
            uploadId=upload_info['uploadId'],
            archiveSize=str(file_size),
            checksum=bin_to_hex(tree_hash_hashes(hashes)))

    # Return archive information
    archive = {
            'archive_id': archive_info['archiveId'],
            'checksum': archive_info['checksum'],
            'filename': os.path.basename(filename),
            'size': file_size,
            'timestamp': int(time.time())
            }
    return archive


def parse_arguments():
    p = argparse.ArgumentParser(
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    default_config = os.path.join(os.path.expanduser('~'),
            DEFAULT_CONFIG_DIR)
    p.add_argument('-c', default=default_config, dest='config_dir',
            help='Path to configuration directory.')
    p.add_argument('filename', type=readable_file_arg, nargs='+',
            help='File to upload for long-term storage.')
    return p.parse_args()


def readable_file_arg(s):
    try:
        open(s, 'rb')
    except:
        raise argparse.ArgumentTypeError(
                "'{}' is not a readable file".format(s))
    return s


def tree_hash_file(filename):
    # File should be hashed in 1 MiB chunks
    chunk_size = 1024 * 1024

    # Create leave nodes by hashing the file
    with open(filename, 'rb') as f:
        hashes = [hashlib.sha256(chunk).digest()
                for chunk in iter(lambda: f.read(chunk_size), b'')]

    # Create tree hash from hashes and return
    return tree_hash_hashes(hashes)


def tree_hash_bytes(b):
    # File should be hashed in 1 MiB chunks
    chunk_size = 1024 * 1024

    # Create leave nodes by hashing the byte sequence
    no_chunks = len(b) // chunk_size if len(b) % chunk_size == 0 else len(b) // chunk_size + 1
    hashes = [hashlib.sha256(b[c * chunk_size:(c + 1) * chunk_size]).digest()
            for c in range(no_chunks)]

    # Create tree hash from hashes and return
    return tree_hash_hashes(hashes)


def tree_hash_hashes(hashes):
    # Recursively move up the tree by hashing consecutive hashes
    while True:
        # Temporarily store last hash if number of hashes is odd
        last = hashes[-1] if len(hashes) % 2 == 1 else None

        # Calculate digest of consecutive hashes
        hashes = [hashlib.sha256(h1 + h2).digest()
                for h1, h2 in zip(hashes[::2], hashes[1::2])]

        # Append stored hash if existing
        if last is not None:
            hashes.append(last)

        # Return hex digest if the root of the tree is reached
        if len(hashes) == 1:
            return hashes[0]


def bin_to_hex(h):
    return binascii.hexlify(h).decode('ascii')


if __name__ == '__main__':
    sys.exit(main())
